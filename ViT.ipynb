{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnr2yOgsS2EFgVryzsIc5O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/torrhen/pytorch_miscellaneous/blob/main/ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "32KvriWfqmYS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  '''\n",
        "  Create patch embeddings using hybrid architecture as described in section 3.1\n",
        "  '''\n",
        "  def __init__(self, in_channels=3, patch_size=16, embedding_dim=768):\n",
        "    super(PatchEmbedding, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.patch_size = patch_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    # create input sequence of patches by flattening the spatial dimensions of the feature map and projecting to the embedding dimension used by the transformer.\n",
        "    self.embedding = nn.Sequential(\n",
        "        # [B, 3, 224, 244] -> [B, 768, 14, 14]\n",
        "        nn.Conv2d(in_channels=self.in_channels, out_channels=self.embedding_dim, kernel_size=self.patch_size, stride=self.patch_size, padding=0),\n",
        "        # [B, 768, 196]\n",
        "        nn.Flatten(start_dim=2, end_dim=3)\n",
        "    )\n",
        "    self.class_token = nn.Parameter(torch.ones(1, 1, self.embedding_dim))\n",
        "    self.position_embedding = nn.Parameter(torch.ones(1, 197, self.embedding_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # input spatial dimensions should be divided without remainder into 16x16 patches\n",
        "    height, width = x.shape[-2:] #  x = [C, H, W]\n",
        "    assert(height % self.patch_size == 0 and width % self.patch_size == 0)\n",
        "    # calculate patch embedding\n",
        "    x = self.embedding(x) # [B, (P . C^2), (HW / P^2)]\n",
        "    x = x.permute(0, 2, 1) # [B, (HW / P^2), (P . C^2)]\n",
        "    # prepend class token to patch embedding\n",
        "    x = torch.cat((self.class_token, x), dim=1)\n",
        "    # add position embedding to patch embedding\n",
        "    x = x + self.position_embedding\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "IQO7UKsoqr1h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self, d=768, h=12, dropout=0):\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "    self.embedding_dim = d\n",
        "    self.h = h\n",
        "    self.dropout = dropout\n",
        "    self.LN = nn.LayerNorm(normalized_shape=self.embedding_dim)\n",
        "    self.MSA = nn.MultiheadAttention(embed_dim=self.embedding_dim,\n",
        "                                     num_heads=self.h,\n",
        "                                     dropout=self.dropout,\n",
        "                                     batch_first=True)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # layer normalization\n",
        "    x = self.LN(x)\n",
        "    # multi-head attention block\n",
        "    output, attn = self.MSA(query=x, key=x, value=x, need_weights=False)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "Eqa6EQblWu5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPHead(nn.Module):\n",
        "  def __init__(self, embedding_dim=768, hidden_units=3072, dropout=0.1):\n",
        "    super(MLPHead, self).__init__()\n",
        "    self.d = embedding_dim\n",
        "    self.mlp_size = hidden_units\n",
        "    self.dropout = dropout\n",
        "    self.LN = nn.LayerNorm(normalized_shape=self.d)\n",
        "    self.MLP = nn.Sequential(\n",
        "        nn.Linear(in_features=self.d, out_features=self.mlp_size),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(p=self.dropout),\n",
        "        nn.Linear(in_features=self.mlp_size, out_features=self.d),\n",
        "        nn.Dropout(p=self.dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # layer normalization\n",
        "    x = self.LN(x)\n",
        "    # MLP head\n",
        "    x = self.MLP(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "SATuAJNwZ7i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, embedding_dim=768, h=12, attn_dropout=0, mlp_size=3072, mlp_dropout=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.d = embedding_dim\n",
        "    self.h = h\n",
        "    self.attn_dropout = attn_dropout\n",
        "    self.mlp_size = mlp_size\n",
        "    self.mlp_dropout = mlp_dropout\n",
        "    # multi head attention\n",
        "    self.MHA = MultiHeadSelfAttention(d=self.d,\n",
        "                                      h=self.h,\n",
        "                                      dropout=self.attn_dropout)\n",
        "    # MLP Head\n",
        "    self.MLP = MLPHead(embedding_dim=self.d,\n",
        "                       hidden_units=self.mlp_size,\n",
        "                       dropout=self.mlp_dropout)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.MHA(x) + x\n",
        "    x = self.MLP(x) + x\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "99ZjfaMqb1O0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}