{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOosCD6AOXXtXICVOJ5fk3d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/torrhen/pytorch_miscellaneous/blob/main/ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "32KvriWfqmYS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  '''\n",
        "  Create patch embeddings using hybrid architecture as described in section 3.1\n",
        "  '''\n",
        "  def __init__(self, in_channels=3, patch_size=16, embedding_dim=768):\n",
        "    super(PatchEmbedding, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.patch_size = patch_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    # create input sequence of patches by flattening the spatial dimensions of the feature map and projecting to the embedding dimension used by the transformer.\n",
        "    self.embedding = nn.Sequential(\n",
        "        # [B, 3, 224, 244] -> [B, 768, 14, 14]\n",
        "        nn.Conv2d(in_channels=self.in_channels, out_channels=self.embedding_dim, kernel_size=self.patch_size, stride=self.patch_size, padding=0),\n",
        "        # [B, 768, 196]\n",
        "        nn.Flatten(start_dim=2, end_dim=3)\n",
        "    )\n",
        "    self.class_token = nn.Parameter(torch.randn(1, 1, self.embedding_dim))\n",
        "    self.position_embedding = nn.Parameter(torch.randn(1, 197, self.embedding_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # input spatial dimensions should be divided without remainder into 16x16 patches\n",
        "    height, width = x.shape[-2:] #  x = [C, H, W]\n",
        "    assert(height % self.patch_size == 0 and width % self.patch_size == 0)\n",
        "    # calculate patch embedding\n",
        "    x = self.embedding(x) # [B, (P . C^2), (HW / P^2)]\n",
        "    x = x.permute(0, 2, 1) # [B, (HW / P^2), (P . C^2)]\n",
        "    # prepend class token to patch embedding\n",
        "    x = torch.cat((self.class_token, x), dim=1)\n",
        "    # add position embedding to patch embedding\n",
        "    x = x + self.position_embedding\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "IQO7UKsoqr1h"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}